{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transforms import *\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "# a= torchvision.ops.nms\n",
    "class RegionProposalNetwork(nn.Module):\n",
    "    r\"\"\"\n",
    "    RPN with following layers on the feature map\n",
    "        1. 3x3 conv layer followed by Relu\n",
    "        2. 1x1 classification conv with num_anchors(num_scales x num_aspect_ratios) output channels\n",
    "        3. 1x1 classification conv with 4 x num_anchors output channels\n",
    "\n",
    "    Classification is done via one value indicating probability of foreground\n",
    "    with sigmoid applied during inference\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, scales, aspect_ratios, model_config):\n",
    "        super(RegionProposalNetwork, self).__init__()\n",
    "        self.scales = scales\n",
    "        self.low_iou_threshold = model_config['rpn_bg_threshold']\n",
    "        self.high_iou_threshold = model_config['rpn_fg_threshold']\n",
    "        self.rpn_nms_threshold = model_config['rpn_nms_threshold']\n",
    "        self.rpn_batch_size = model_config['rpn_batch_size']\n",
    "        self.rpn_pos_count = int(model_config['rpn_pos_fraction'] * self.rpn_batch_size)\n",
    "        self.rpn_topk = model_config['rpn_train_topk'] if self.training else model_config['rpn_test_topk']\n",
    "        self.rpn_prenms_topk = model_config['rpn_train_prenms_topk'] if self.training \\\n",
    "            else model_config['rpn_test_prenms_topk']\n",
    "        self.aspect_ratios = aspect_ratios\n",
    "        self.num_anchors = len(self.scales) * len(self.aspect_ratios)\n",
    "        \n",
    "        # 3x3 conv layer\n",
    "        self.rpn_conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # 1x1 classification conv layer\n",
    "        self.cls_layer = nn.Conv2d(in_channels, self.num_anchors, kernel_size=1, stride=1)\n",
    "        \n",
    "        # 1x1 regression\n",
    "        self.bbox_reg_layer = nn.Conv2d(in_channels, self.num_anchors * 4, kernel_size=1, stride=1)\n",
    "        \n",
    "        for layer in [self.rpn_conv, self.cls_layer, self.bbox_reg_layer]:\n",
    "            torch.nn.init.normal_(layer.weight, std=0.01)\n",
    "            torch.nn.init.constant_(layer.bias, 0)\n",
    "    \n",
    "    def generate_anchors(self, image, feat):\n",
    "        r\"\"\"\n",
    "        Method to generate anchors. First we generate one set of zero-centred anchors\n",
    "        using the scales and aspect ratios provided.\n",
    "        We then generate shift values in x,y axis for all featuremap locations.\n",
    "        The single zero centred anchors generated are replicated and shifted accordingly\n",
    "        to generate anchors for all feature map locations.\n",
    "        Note that these anchors are generated such that their centre is top left corner of the\n",
    "        feature map cell rather than the centre of the feature map cell.\n",
    "        :param image: (N, C, H, W) tensor\n",
    "        :param feat: (N, C_feat, H_feat, W_feat) tensor\n",
    "        :return: anchor boxes of shape (H_feat * W_feat * num_anchors_per_location, 4)\n",
    "        \"\"\"\n",
    "        grid_h, grid_w = feat.shape[-2:]\n",
    "        image_h, image_w = image.shape[-2:]\n",
    "        \n",
    "        # For the vgg16 case stride would be 16 for both h and w\n",
    "        stride_h = torch.tensor(image_h // grid_h, dtype=torch.int64, device=feat.device)\n",
    "        stride_w = torch.tensor(image_w // grid_w, dtype=torch.int64, device=feat.device)\n",
    "        \n",
    "        scales = torch.as_tensor(self.scales, dtype=feat.dtype, device=feat.device)\n",
    "        aspect_ratios = torch.as_tensor(self.aspect_ratios, dtype=feat.dtype, device=feat.device)\n",
    "        \n",
    "        # Assuming anchors of scale 128 sq pixels\n",
    "        # For 1:1 it would be (128, 128) -> area=16384\n",
    "        # For 2:1 it would be (181.02, 90.51) -> area=16384\n",
    "        # For 1:2 it would be (90.51, 181.02) -> area=16384\n",
    "        \n",
    "        # The below code ensures h/w = aspect_ratios and h*w=1\n",
    "        h_ratios = torch.sqrt(aspect_ratios)\n",
    "        w_ratios = 1 / h_ratios\n",
    "        \n",
    "        # Now we will just multiply h and w with scale(example 128)\n",
    "        # to make h*w = 128 sq pixels and h/w = aspect_ratios\n",
    "        # This gives us the widths and heights of all anchors\n",
    "        # which we need to replicate at all locations\n",
    "        ws = (w_ratios[:, None] * scales[None, :]).view(-1)\n",
    "        hs = (h_ratios[:, None] * scales[None, :]).view(-1)\n",
    "        \n",
    "        # Now we make all anchors zero centred\n",
    "        # So x1, y1, x2, y2 = -w/2, -h/2, w/2, h/2\n",
    "        base_anchors = torch.stack([-ws, -hs, ws, hs], dim=1) / 2\n",
    "        base_anchors = base_anchors.round()\n",
    "        \n",
    "        # Get the shifts in x axis (0, 1,..., W_feat-1) * stride_w\n",
    "        shifts_x = torch.arange(0, grid_w, dtype=torch.int32, device=feat.device) * stride_w\n",
    "\n",
    "        # Get the shifts in x axis (0, 1,..., H_feat-1) * stride_h\n",
    "        shifts_y = torch.arange(0, grid_h, dtype=torch.int32, device=feat.device) * stride_h\n",
    "        \n",
    "        # Create a grid using these shifts\n",
    "        shifts_y, shifts_x = torch.meshgrid(shifts_y, shifts_x, indexing=\"ij\")\n",
    "        # shifts_x -> (H_feat, W_feat)\n",
    "        # shifts_y -> (H_feat, W_feat)\n",
    "        \n",
    "        shifts_x = shifts_x.reshape(-1)\n",
    "        shifts_y = shifts_y.reshape(-1)\n",
    "        # Setting shifts for x1 and x2(same as shifts_x) and y1 and y2(same as shifts_y)\n",
    "        shifts = torch.stack((shifts_x, shifts_y, shifts_x, shifts_y), dim=1)\n",
    "        # shifts -> (H_feat * W_feat, 4)\n",
    "        \n",
    "        # base_anchors -> (num_anchors_per_location, 4)\n",
    "        # shifts -> (H_feat * W_feat, 4)\n",
    "        # Add these shifts to each of the base anchors\n",
    "        anchors = (shifts.view(-1, 1, 4) + base_anchors.view(1, -1, 4))\n",
    "        # anchors -> (H_feat * W_feat, num_anchors_per_location, 4)\n",
    "        anchors = anchors.reshape(-1, 4)\n",
    "        # anchors -> (H_feat * W_feat * num_anchors_per_location, 4)\n",
    "        return anchors\n",
    "    \n",
    "    def assign_targets_to_anchors(self, anchors, gt_boxes):\n",
    "        r\"\"\"\n",
    "        For each anchor assign a ground truth box based on the IOU.\n",
    "        Also creates classification labels to be used for training\n",
    "        label=1 for anchors where maximum IOU with a gtbox > high_iou_threshold\n",
    "        label=0 for anchors where maximum IOU with a gtbox < low_iou_threshold\n",
    "        label=-1 for anchors where maximum IOU with a gtbox between (low_iou_threshold, high_iou_threshold)\n",
    "        :param anchors: (num_anchors_in_image, 4) all anchor boxes\n",
    "        :param gt_boxes: (num_gt_boxes_in_image, 4) all ground truth boxes\n",
    "        :return:\n",
    "            label: (num_anchors_in_image) {-1/0/1}\n",
    "            matched_gt_boxes: (num_anchors_in_image, 4) coordinates of assigned gt_box to each anchor\n",
    "                Even background/to_be_ignored anchors will be assigned some ground truth box.\n",
    "                It's fine, we will use label to differentiate those instances later\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get (gt_boxes, num_anchors_in_image) IOU matrix\n",
    "        iou_matrix = get_iou(gt_boxes, anchors)\n",
    "        \n",
    "        # For each anchor get the gt box index with maximum overlap\n",
    "        best_match_iou, best_match_gt_idx = iou_matrix.max(dim=0)\n",
    "        # best_match_gt_idx -> (num_anchors_in_image)\n",
    "        \n",
    "        # This copy of best_match_gt_idx will be needed later to\n",
    "        # add low quality matches\n",
    "        best_match_gt_idx_pre_thresholding = best_match_gt_idx.clone()\n",
    "        \n",
    "        # Based on threshold, update the values of best_match_gt_idx\n",
    "        # For anchors with highest IOU < low_threshold update to be -1\n",
    "        # For anchors with highest IOU between low_threshold & high threshold update to be -2\n",
    "        below_low_threshold = best_match_iou < self.low_iou_threshold\n",
    "        between_thresholds = (best_match_iou >= self.low_iou_threshold) & (best_match_iou < self.high_iou_threshold)\n",
    "        best_match_gt_idx[below_low_threshold] = -1\n",
    "        best_match_gt_idx[between_thresholds] = -2\n",
    "        \n",
    "        # Add low quality anchor boxes, if for a given ground truth box, these are the ones\n",
    "        # that have highest IOU with that gt box\n",
    "        \n",
    "        # For each gt box, get the maximum IOU value amongst all anchors\n",
    "        best_anchor_iou_for_gt, _ = iou_matrix.max(dim=1)\n",
    "        # best_anchor_iou_for_gt -> (num_gt_boxes_in_image)\n",
    "        \n",
    "        # For each gt box get those anchors\n",
    "        # which have this same IOU as present in best_anchor_iou_for_gt\n",
    "        # This is to ensure if 10 anchors all have the same IOU value,\n",
    "        # which is equal to the highest IOU that this gt box has with any anchor\n",
    "        # then we get all these 10 anchors\n",
    "        gt_pred_pair_with_highest_iou = torch.where(iou_matrix == best_anchor_iou_for_gt[:, None])\n",
    "        # gt_pred_pair_with_highest_iou -> [0, 0, 0, 1, 1, 1], [8896,  8905,  8914, 10472, 10805, 11138]\n",
    "        # This means that anchors at the first 3 indexes have an IOU with gt box at index 0\n",
    "        # which is equal to the highest IOU that this gt box has with ANY anchor\n",
    "        # Similarly anchor at last three indexes(10472, 10805, 11138) have an IOU with gt box at index 1\n",
    "        # which is equal to the highest IOU that this gt box has with ANY anchor\n",
    "        # These 6 anchor indexes will also be added as positive anchors\n",
    "        \n",
    "        # Get all the anchors indexes to update\n",
    "        pred_inds_to_update = gt_pred_pair_with_highest_iou[1]\n",
    "        \n",
    "        # Update the matched gt index for all these anchors with whatever was the best gt box\n",
    "        # prior to thresholding\n",
    "        best_match_gt_idx[pred_inds_to_update] = best_match_gt_idx_pre_thresholding[pred_inds_to_update]\n",
    "        \n",
    "        # best_match_gt_idx is either a valid index for all anchors or -1(background) or -2(to be ignored)\n",
    "        # Clamp this so that the best_match_gt_idx is a valid non-negative index\n",
    "        # At this moment the -1 and -2 labelled anchors will be mapped to the 0th gt box\n",
    "        matched_gt_boxes = gt_boxes[best_match_gt_idx.clamp(min=0)]\n",
    "        \n",
    "        # Set all foreground anchor labels as 1\n",
    "        labels = best_match_gt_idx >= 0\n",
    "        labels = labels.to(dtype=torch.float32)\n",
    "        \n",
    "        # Set all background anchor labels as 0\n",
    "        background_anchors = best_match_gt_idx == -1\n",
    "        labels[background_anchors] = 0.0\n",
    "        \n",
    "        # Set all to be ignored anchor labels as -1\n",
    "        ignored_anchors = best_match_gt_idx == -2\n",
    "        labels[ignored_anchors] = -1.0\n",
    "        # Later for classification we will only pick labels which have > 0 label\n",
    "        \n",
    "        return labels, matched_gt_boxes\n",
    "\n",
    "    def filter_proposals(self, proposals, cls_scores, image_shape):\n",
    "        r\"\"\"\n",
    "        This method does three kinds of filtering/modifications\n",
    "        1. Pre NMS topK filtering\n",
    "        2. Make proposals valid by clamping coordinates(0, width/height)\n",
    "        2. Small Boxes filtering based on width and height\n",
    "        3. NMS\n",
    "        4. Post NMS topK filtering\n",
    "        :param proposals: (num_anchors_in_image, 4)\n",
    "        :param cls_scores: (num_anchors_in_image, 4) these are cls logits\n",
    "        :param image_shape: resized image shape needed to clip proposals to image boundary\n",
    "        :return: proposals and cls_scores: (num_filtered_proposals, 4) and (num_filtered_proposals)\n",
    "        \"\"\"\n",
    "        # Pre NMS Filtering\n",
    "        cls_scores = cls_scores.reshape(-1)\n",
    "        cls_scores = torch.sigmoid(cls_scores)\n",
    "        _, top_n_idx = cls_scores.topk(min(self.rpn_prenms_topk, len(cls_scores)))\n",
    "        \n",
    "        cls_scores = cls_scores[top_n_idx]\n",
    "        proposals = proposals[top_n_idx]\n",
    "        ##################\n",
    "        \n",
    "        # Clamp boxes to image boundary\n",
    "        proposals = clamp_boxes_to_image_boundary(proposals, image_shape)\n",
    "        ####################\n",
    "        \n",
    "        # Small boxes based on width and height filtering\n",
    "        min_size = 16\n",
    "        ws, hs = proposals[:, 2] - proposals[:, 0], proposals[:, 3] - proposals[:, 1]\n",
    "        keep = (ws >= min_size) & (hs >= min_size)\n",
    "        keep = torch.where(keep)[0]\n",
    "        proposals = proposals[keep]\n",
    "        cls_scores = cls_scores[keep]\n",
    "        ####################\n",
    "        \n",
    "        # NMS based on objectness scores\n",
    "        keep_mask = torch.zeros_like(cls_scores, dtype=torch.bool)\n",
    "        keep_indices = torchvision.ops.nms(proposals, cls_scores, self.rpn_nms_threshold)\n",
    "        keep_mask[keep_indices] = True\n",
    "        keep_indices = torch.where(keep_mask)[0]\n",
    "        # Sort by objectness\n",
    "        post_nms_keep_indices = keep_indices[cls_scores[keep_indices].sort(descending=True)[1]]\n",
    "        \n",
    "        # Post NMS topk filtering\n",
    "        proposals, cls_scores = (proposals[post_nms_keep_indices[:self.rpn_topk]],\n",
    "                                 cls_scores[post_nms_keep_indices[:self.rpn_topk]])\n",
    "        \n",
    "        return proposals, cls_scores\n",
    "    \n",
    "    def forward(self, image, feat, target=None):\n",
    "        r\"\"\"\n",
    "        Main method for RPN does the following:\n",
    "        1. Call RPN specific conv layers to generate classification and\n",
    "            bbox transformation predictions for anchors\n",
    "        2. Generate anchors for entire image\n",
    "        3. Transform generated anchors based on predicted bbox transformation to generate proposals\n",
    "        4. Filter proposals\n",
    "        5. For training additionally we do the following:\n",
    "            a. Assign target ground truth labels and boxes to each anchors\n",
    "            b. Sample positive and negative anchors\n",
    "            c. Compute classification loss using sampled pos/neg anchors\n",
    "            d. Compute Localization loss using sampled pos anchors\n",
    "        :param image:\n",
    "        :param feat:\n",
    "        :param target:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # Call RPN layers\n",
    "        rpn_feat = nn.ReLU()(self.rpn_conv(feat))\n",
    "        cls_scores = self.cls_layer(rpn_feat)\n",
    "        box_transform_pred = self.bbox_reg_layer(rpn_feat)\n",
    "\n",
    "        # Generate anchors\n",
    "        anchors = self.generate_anchors(image, feat)\n",
    "        \n",
    "        # Reshape classification scores to be (Batch Size * H_feat * W_feat * Number of Anchors Per Location, 1)\n",
    "        # cls_score -> (Batch_Size, Number of Anchors per location, H_feat, W_feat)\n",
    "        number_of_anchors_per_location = cls_scores.size(1)\n",
    "        cls_scores = cls_scores.permute(0, 2, 3, 1)\n",
    "        cls_scores = cls_scores.reshape(-1, 1)\n",
    "        # cls_score -> (Batch_Size*H_feat*W_feat*Number of Anchors per location, 1)\n",
    "        \n",
    "        # Reshape bbox predictions to be (Batch Size * H_feat * W_feat * Number of Anchors Per Location, 4)\n",
    "        # box_transform_pred -> (Batch_Size, Number of Anchors per location*4, H_feat, W_feat)\n",
    "        box_transform_pred = box_transform_pred.view(\n",
    "            box_transform_pred.size(0),\n",
    "            number_of_anchors_per_location,\n",
    "            4,\n",
    "            rpn_feat.shape[-2],\n",
    "            rpn_feat.shape[-1])\n",
    "        box_transform_pred = box_transform_pred.permute(0, 3, 4, 1, 2)\n",
    "        box_transform_pred = box_transform_pred.reshape(-1, 4)\n",
    "        # box_transform_pred -> (Batch_Size*H_feat*W_feat*Number of Anchors per location, 4)\n",
    "        \n",
    "        # Transform generated anchors according to box transformation prediction\n",
    "        proposals = apply_regression_pred_to_anchors_or_proposals(\n",
    "            box_transform_pred.detach().reshape(-1, 1, 4),\n",
    "            anchors)\n",
    "        proposals = proposals.reshape(proposals.size(0), 4)\n",
    "        ######################\n",
    "        \n",
    "        proposals, scores = self.filter_proposals(proposals, cls_scores.detach(), image.shape)\n",
    "        rpn_output = {\n",
    "            'proposals': proposals,\n",
    "            'scores': scores\n",
    "        }\n",
    "        if not self.training or target is None:\n",
    "            # If we are not training no need to do anything\n",
    "            return rpn_output\n",
    "        else:\n",
    "            # Assign gt box and label for each anchor\n",
    "            labels_for_anchors, matched_gt_boxes_for_anchors = self.assign_targets_to_anchors(\n",
    "                anchors,\n",
    "                target['bboxes'][0])\n",
    "            \n",
    "            # Based on gt assignment above, get regression target for the anchors\n",
    "            # matched_gt_boxes_for_anchors -> (Number of anchors in image, 4)\n",
    "            # anchors -> (Number of anchors in image, 4)\n",
    "            regression_targets = boxes_to_transformation_targets(matched_gt_boxes_for_anchors, anchors)\n",
    "            \n",
    "            ####### Sampling positive and negative anchors ####\n",
    "            # Our labels were {fg:1, bg:0, to_be_ignored:-1}\n",
    "            sampled_neg_idx_mask, sampled_pos_idx_mask = sample_positive_negative(\n",
    "                labels_for_anchors,\n",
    "                positive_count=self.rpn_pos_count,\n",
    "                total_count=self.rpn_batch_size)\n",
    "            \n",
    "            sampled_idxs = torch.where(sampled_pos_idx_mask | sampled_neg_idx_mask)[0]\n",
    "            \n",
    "            localization_loss = (\n",
    "                    torch.nn.functional.smooth_l1_loss(\n",
    "                        box_transform_pred[sampled_pos_idx_mask],\n",
    "                        regression_targets[sampled_pos_idx_mask],\n",
    "                        beta=1 / 9,\n",
    "                        reduction=\"sum\",\n",
    "                    )\n",
    "                    / (sampled_idxs.numel())\n",
    "            ) \n",
    "\n",
    "            cls_loss = torch.nn.functional.binary_cross_entropy_with_logits(cls_scores[sampled_idxs].flatten(),\n",
    "                                                                            labels_for_anchors[sampled_idxs].flatten())\n",
    "\n",
    "            rpn_output['rpn_classification_loss'] = cls_loss\n",
    "            rpn_output['rpn_localization_loss'] = localization_loss\n",
    "            return rpn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proposals: torch.Size([160, 4])\n",
      "Scores: torch.Size([160])\n",
      "RPN Classification Loss: tensor(0.7020, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "RPN Localization Loss: tensor(0.2503, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define a sample configuration dictionary as required by the RPN\n",
    "model_config = {\n",
    "    'rpn_bg_threshold': 0.3,\n",
    "    'rpn_fg_threshold': 0.7,\n",
    "    'rpn_nms_threshold': 0.7,\n",
    "    'rpn_batch_size': 256,\n",
    "    'rpn_pos_fraction': 0.5,\n",
    "    'rpn_train_topk': 2000,\n",
    "    'rpn_test_topk': 1000,\n",
    "    'rpn_train_prenms_topk': 12000,\n",
    "    'rpn_test_prenms_topk': 6000\n",
    "}\n",
    "\n",
    "# Instantiate the RegionProposalNetwork with arbitrary in_channels, scales, and aspect_ratios\n",
    "in_channels = 512  # Example: this might be from a VGG16 feature map\n",
    "scales = [128, 256, 512]\n",
    "aspect_ratios = [0.5, 1.0, 2.0]\n",
    "rpn = RegionProposalNetwork(in_channels, scales, aspect_ratios, model_config)\n",
    "\n",
    "# Create a sample image tensor (batch size, channels, height, width)\n",
    "# and a feature map tensor from a backbone network (batch size, channels, height, width)\n",
    "image = torch.randn((1, 3, 224, 224))  # Typical input image for VGG16\n",
    "feat = torch.randn((1, in_channels, 14, 14))  # Example feature map size from VGG-like architecture\n",
    "\n",
    "# Sample ground truth boxes in target dictionary for training\n",
    "# Assumes single image, with two ground truth boxes, format (x1, y1, x2, y2)\n",
    "target = {\n",
    "    'bboxes': torch.tensor([[[50, 50, 150, 150], [30, 30, 70, 90]]], dtype=torch.float32)\n",
    "}\n",
    "\n",
    "# Run the RPN forward pass in training mode with the target data\n",
    "rpn.train()  # Ensure the model is in training mode to apply training-specific configurations\n",
    "output = rpn(image, feat, target)\n",
    "\n",
    "# Print output to verify correctness\n",
    "print(\"Proposals:\", output['proposals'].shape)\n",
    "print(\"Scores:\", output['scores'].shape)\n",
    "print(\"RPN Classification Loss:\", output.get('rpn_classification_loss', 'N/A'))\n",
    "print(\"RPN Localization Loss:\", output.get('rpn_localization_loss', 'N/A'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
